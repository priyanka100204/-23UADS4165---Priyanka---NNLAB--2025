{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBJECTIVE : WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.\n",
    "\n",
    "Model Description: Architecture: \n",
    "Input Layer (784 neurons) : Takes flattened 28×28 grayscale images.\n",
    "Hidden Layer 1 (128 neurons) : Sigmoid activation.\n",
    "Hidden Layer 2 (64 neurons) : Sigmoid activation.\n",
    "Output Layer (10 neurons) : Produces logits, passed to Softmax for classification.\n",
    "Hyperparameters:-\n",
    "Loss Function: Softmax Cross-Entropy \n",
    "Optimizer: Adam \n",
    "Learning Rate: 0.01 \n",
    "Batch Size: 100 \n",
    "Epochs: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Define forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n",
    "\n",
    "# Loss function (Softmax Cross-Entropy)\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "# Optimizer (Adam)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Compute final training and test accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (Adam): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (Adam): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1, Loss: 173.6847\n",
    "Epoch 2, Loss: 65.5254\n",
    "Epoch 3, Loss: 50.9543\n",
    "Epoch 4, Loss: 39.6772\n",
    "Epoch 5, Loss: 33.9670\n",
    "Epoch 6, Loss: 30.5116\n",
    "Epoch 7, Loss: 26.5907\n",
    "Epoch 8, Loss: 24.5352\n",
    "Epoch 9, Loss: 24.4435\n",
    "Epoch 10, Loss: 22.0982\n",
    "Final Training Accuracy (Adam): 0.9906\n",
    "Final Test Accuracy (Adam): 0.9738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE DESCRIPTION:\n",
    "\n",
    "1. Dataset Loading & Preprocessing: -The MNIST dataset is loaded using tensorflow_datasets. -Images are normalized (values scaled between 0 and 1) for better training stability. -Labels are one-hot encoded -The dataset is shuffled and batched to improve training efficiency. -Each image is flattened from 28×28 to a 1D vector of 784 values.\n",
    "\n",
    "2. Model Definition: -Weights (W) and biases (b) are initialized randomly for each layer. -The forward pass involves matrix multiplications and sigmoid activation in the hidden layers. -The output layer produces logits (raw scores before applying softmax).\n",
    "\n",
    "3. Loss Function: -The loss function is Softmax Cross-Entropy, which measures how far the predicted probability distribution is from the actual label distribution.\n",
    "\n",
    "4. Accuracy Computation: -Accuracy is computed by checking how many predicted labels match the actual labels.\n",
    "\n",
    "5. Optimization (Adam Optimizer): -Adam optimizer is used to update weights. It adapts the learning rate dynamically to improve training speed and convergence.\n",
    "\n",
    "6. Training Loop: -The model is trained for 10 epochs. -In each epoch, all training samples are passed through the model, and the weights are updated based on the computed loss. -At the end of training, accuracy is computed for both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY COMMENTS:\n",
    "Adam optimizer's learning rate must be low. -Use ReLU instead of Sigmoid. -It works better with greater batch size -Faster Compared to SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Accuracy: 0.9906 Test Accuracy: 0.9738"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
